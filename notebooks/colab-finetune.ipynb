{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "medical-assistant-finetune"
   },
   "source": [
    "# AlpaCare Medical Assistant - Fine-tuning Notebook\n",
    "## Solar Industries India Ltd - Internship Assessment\n",
    "\n",
    "This notebook fine-tunes a pre-trained model on medical instructions using LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-install"
   },
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets peft accelerate bitsandbytes einops trl scipy safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from data_loader import load_medical_dataset, preprocess_function\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-setup"
   },
   "source": [
    "## 3. Model & Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-model"
   },
   "outputs": [],
   "source": [
    "# Model choice: Microsoft DialoGPT-medium - Small, fast, good for conversations\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora-config"
   },
   "source": [
    "## 4. LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lora-setup"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]  # GPT-2 attention modules\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"LoRA configuration applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading"
   },
   "source": [
    "## 5. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_splits = load_medical_dataset()\n",
    "\n",
    "# Use smaller subset for demo (full training in actual scenario)\n",
    "train_dataset = dataset_splits['train'].select(range(1000))  # First 1000 samples for demo\n",
    "eval_dataset = dataset_splits['validation'].select(range(200))  # 200 samples for eval\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Preprocess data\n",
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-args"
   },
   "source": [
    "## 6. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./alpacare-medical-assistant\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # One epoch as required\n",
    "    per_device_train_batch_size=4,  # Small batch for Colab memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb/etc\n",
    "    fp16=True,  # Enable mixed precision\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trainer-setup"
   },
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer-init"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized! Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start-training"
   },
   "source": [
    "## 8. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-model"
   },
   "source": [
    "## 9. Save LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-adapter"
   },
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_path = \"./alpacare-lora-adapter\"\n",
    "trainer.save_model(adapter_path)\n",
    "\n",
    "# Also save via PeftModel to be safe\n",
    "model.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Create zip file for download\n",
    "!zip -r alpacare-lora-adapter.zip {adapter_path}\n",
    "\n",
    "print(\"Adapter zip file created! You can download it from Colab files section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "completion"
   },
   "source": [
    "## âœ… Training Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download `alpacare-lora-adapter.zip` from Colab files\n",
    "2. Use the inference notebook to test the model\n",
    "3. Upload adapter to your GitHub repository"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
